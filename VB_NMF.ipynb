{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdzX-Yn26zvx"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(io.BytesIO(uploaded['dataFilt.csv'], index_col=0))\n",
        "pathways = pd.read_csv(io.BytesIO(uploaded['kegg_legacy_ensembl.csv'], index_col=0))\n",
        "sample_classes = pd.read_csv(io.BytesIO(uploaded['sampletype.csv'], index_col=0))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.values, dtype=torch.float32)\n",
        "\n",
        "# Implement VB-NMF model\n",
        "class VBNMF(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(VBNMF, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.W = nn.Parameter(torch.randn(input_dim, latent_dim))\n",
        "        self.H = nn.Parameter(torch.randn(latent_dim, input_dim))\n",
        "        self.a = nn.Parameter(torch.randn(input_dim))\n",
        "        self.b = nn.Parameter(torch.randn(input_dim))\n",
        "\n",
        "    def forward(self, X, n_samples=1):\n",
        "        KL_loss = 0\n",
        "        recon_loss = 0\n",
        "        for _ in range(n_samples):\n",
        "            Q_W = torch.distributions.Normal(0, 1).sample(self.W.shape).to(X.device)\n",
        "            Q_H = torch.distributions.Normal(0, 1).sample(self.H.shape).to(X.device)\n",
        "            W_sample = self.W + Q_W\n",
        "            H_sample = self.H + Q_H\n",
        "            recon = torch.matmul(W_sample, H_sample)\n",
        "            recon_loss += F.binary_cross_entropy_with_logits(recon, X, reduction='sum')\n",
        "            KL_loss += 0.5 * torch.sum(self.W**2 + self.H**2 - 1 - self.a.log() - self.b.log())\n",
        "\n",
        "        recon_loss /= n_samples\n",
        "        KL_loss /= n_samples\n",
        "        ELBO = recon_loss + KL_loss\n",
        "        return ELBO\n",
        "\n",
        "# Example usage\n",
        "input_dim = data.shape[1]\n",
        "latent_dim = 10\n",
        "model = VBNMF(input_dim, latent_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(X, n_samples=5)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch:03d}, Loss {loss:.4f}')\n",
        "\n",
        "# Use pathway information and sample classes for validation\n",
        "# You can use clustering results from the VB-NMF model to validate against pathway information and sample classes\n",
        "# For example, you can compute the overlap between clusters and pathways, or use sample classes for cluster validation"
      ],
      "metadata": {
        "id": "MT-3IBz2685J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}